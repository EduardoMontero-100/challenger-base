{"paragraphs":[{"text":"%livy.pyspark\n\ndef BorrarTablasTemporales():\n\n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_0' )\n    except:\n        pass\n\n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_1' )\n    except:\n        pass\n\n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_2' )\n    except:\n        pass\n\n\n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_testing' )\n    except:\n        pass\n    \n    \n    \n    # Estas 2 son las que hay que grabar....\n    \n    \n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_metricas' )\n    except:\n        pass\n    \n    \n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_feature_importance' )\n    except:\n        pass\n    \n    \n    try:\n        spark.sql(' drop table sdb_datamining.' + modelo + '_feature_importance_rank' )\n    except:\n        pass\n    \n    ","user":"u632820","dateUpdated":"2022-11-24T02:19:08-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228365802_1152061862","id":"20221107-151752_1530472399","dateCreated":"2022-11-23T15:32:45-0300","dateStarted":"2022-11-24T02:19:08-0300","dateFinished":"2022-11-24T02:19:09-0300","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:183"},{"text":"%livy.pyspark\n\ndef BalancearABT(train_df,  pBalanceo):\n    \n    import pyspark.sql.functions as F\n    ### Undersampling\n    # Realizamos undersampling para balancear las clases 0 y 1 del target del dataset de training, quedando una relacion 1 a 20\n\n    sample0 = train_df.filter(F.col(TARGET) == 0).count()\n    sample1 = train_df.filter(F.col(TARGET) == 1).count()\n\n    if(sample0>=sample1):\n        major_df = train_df.filter(F.col(TARGET) == 0)\n        minor_df = train_df.filter(F.col(TARGET) == 1)\n    else:\n        major_df = train_df.filter(F.col(TARGET) == 1)\n        minor_df = train_df.filter(F.col(TARGET) == 0)\n\n\n    ratio = int(major_df.count()/minor_df.count())\n\n    sampled_majority_df = major_df.sample(False, pBalanceo/ratio, seed=1234)\n    train_undersampled_df = sampled_majority_df.unionAll(minor_df)\n\n    # Aca empieza el codigo de Sebastian\n    print('Data Frame 1: ', train_undersampled_df.count())\n\n    return train_undersampled_df","user":"u632820","dateUpdated":"2022-11-24T02:19:09-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228365987_-1998578778","id":"20221104-094150_1695279521","dateCreated":"2022-11-23T15:32:45-0300","dateStarted":"2022-11-24T02:19:09-0300","dateFinished":"2022-11-24T02:19:10-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:184"},{"text":"%livy.pyspark\n\ndef CastBigInt(train_undersampled_df):\n    \n    \n    import pandas as pd\n    \n    a = pd.DataFrame(train_undersampled_df.dtypes)\n    a.columns = ['columna', 'tipo']\n    \n    print(a.tipo.value_counts())\n    \n    print(list(a[(a.tipo == 'bigint') & (a.columna != CAMPO_CLAVE)].columna))\n    \n    from pyspark.sql.types import ShortType\n    \n    import pyspark.sql.functions as F\n    \n    # Si no se quiere que todas las bigint pasen a ShortType cambiar variables_bigint\n    \n    variables_bigint = list(a[a.tipo == 'bigint'].columna)\n    \n    \"\"\"\n        Cuanto mas chico el nro mejor \n        https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n        Numeric types\n            ByteType: Represents 1-byte signed integer numbers. The range of numbers is from -128 to 127.\n            ShortType: Represents 2-byte signed integer numbers. The range of numbers is from -32768 to 32767.\n            IntegerType: Represents 4-byte signed integer numbers. The range of numbers is from -2147483648 to 2147483647.\n    \"\"\"\n        \n    for c_name in variables_bigint :\n        # print(c_name)\n        train_undersampled_df = train_undersampled_df.withColumn(c_name, F.col(c_name).cast(ShortType()))\n        \n    return train_undersampled_df","user":"u632820","dateUpdated":"2022-11-24T02:19:10-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228366168_-911430294","id":"20221104-094227_348840938","dateCreated":"2022-11-23T15:32:46-0300","dateStarted":"2022-11-24T02:19:11-0300","dateFinished":"2022-11-24T02:19:12-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"text":"%livy.pyspark\n\ndef RedondearDecimales(train_undersampled_df, pDecimales):\n    import pyspark.sql.functions as F\n    # Redondeo decimales\n    # Numerical vars\n    numericCols = [c for c in train_undersampled_df.columns if c not in [CAMPO_CLAVE,'periodo', 'origin', 'label']]\n    print(\"Num. numeric vars: \" , len(numericCols))\n    \n    for c_name, c_type in train_undersampled_df.dtypes:\n        #if c_type in ('double', 'float', 'decimal', 'int', 'smallint'):\n        train_undersampled_df = train_undersampled_df.withColumn(c_name, F.round(c_name, pDecimales))\n    \n    return train_undersampled_df","user":"u632820","dateUpdated":"2022-11-24T02:19:12-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228366353_-1906920367","id":"20221104-094246_1545721886","dateCreated":"2022-11-23T15:32:46-0300","dateStarted":"2022-11-24T02:19:12-0300","dateFinished":"2022-11-24T02:19:13-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"text":"%livy.pyspark\n\ndef EliminarCorrelaciones(train_undersampled_df, pCota):\n        \n    # Saco Columnas Correlacionadas\n\n    # Numerical vars\n    numericCols = [c for c in train_undersampled_df.columns if c not in [CAMPO_CLAVE,'periodo', 'origin', 'label']]\n    \n    print(\"Num. numeric vars: \" , len(numericCols))\n    \n    # Saco correlaciones con un 10% de la base en Pandas, :(\n    \n    import numpy as np\n    \n    df = train_undersampled_df.sample(fraction=0.2, seed=1234).toPandas()\n    \n    # Create correlation matrix\n    corr_matrix = df.corr().abs()\n    \n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    \n    # Find features with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(upper[column] > pCota)]\n    \n    # Drop features \n    print(to_drop)\n    # df.drop(to_drop, axis=1, inplace=True)\n    \n    \n    print('*'*20)\n    print('Variables a eliminar: ', len(to_drop))\n    \n    train_undersampled_df = train_undersampled_df.drop(*to_drop)\n    \n    print('Variables finales: ', len(train_undersampled_df.columns))\n    \n    return train_undersampled_df","user":"u632820","dateUpdated":"2022-11-24T02:19:13-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228366544_1964277732","id":"20221104-094407_1636984023","dateCreated":"2022-11-23T15:32:46-0300","dateStarted":"2022-11-24T02:19:13-0300","dateFinished":"2022-11-24T02:19:14-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"text":"%livy.pyspark\n\ndef ControlParticiones(train_undersampled_df, Cantidad_de_Particiones):\n    # Control de Particiones.....\n    import pyspark.sql.functions as F\n    \n    print(\"Number of partitions PARTY: {}\".format(train_undersampled_df.rdd.getNumPartitions()))\n    \n    print(train_undersampled_df.count())\n        \n    # Particiono en 4 Partes, aca particionar en Pares dependiendo del tamaño, no muy bajo y no muy grande cada particion....\n    # Mas de 100k y menos de 300k cada particion\n    \n    # Ni muy chica, ni muy grande cada particion\n    train_undersampled_df = train_undersampled_df.repartition(Cantidad_de_Particiones, CAMPO_CLAVE)\n    train_undersampled_df.groupBy(F.spark_partition_id()).count().show()\n    \n    return train_undersampled_df","user":"u632820","dateUpdated":"2022-11-24T02:19:14-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228366739_489592764","id":"20221104-094914_595041105","dateCreated":"2022-11-23T15:32:46-0300","dateStarted":"2022-11-24T02:19:15-0300","dateFinished":"2022-11-24T02:19:16-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"text":"%livy.pyspark\n\n    \ndef EntrenarModeloSpark(pALGORITHM, train_undersampled_df, Training_Porcentaje, parametros, Nombre_Modelo):\n#Training_Porcentaje = 0.2\n#pALGORITHM = 'RF'\n#if 1 > 0:\n    import pandas as pd\n    import pyspark.sql.functions as F\n    \n    numTrees = parametros['numTrees']\n    maxIter = parametros['maxIter']\n    maxDepth = parametros['maxDepth']\n    minInstancesPerNode  = parametros['minInstancesPerNode']\n    maxBins = parametros['maxBins']\n    \n    from pyspark.ml.tuning import ParamGridBuilder\n        \n    if pALGORITHM == 'RF':\n        print('RANDOM FOREST')\n        from pyspark.ml.classification import RandomForestClassifier\n        \n        if CON_SCALER == True:\n            model = RandomForestClassifier(labelCol=\"label_\", featuresCol=\"features_scaled\", seed=12345)\n        else:\n            model = RandomForestClassifier(labelCol=\"label_\", featuresCol=\"features\", seed=12345)\n         \n        paramGrid = ParamGridBuilder() \\\n            .addGrid(model.numTrees, numTrees) \\\n            .addGrid(model.maxDepth, maxDepth) \\\n            .addGrid(model.minInstancesPerNode, minInstancesPerNode) \\\n            .addGrid(model.maxBins, maxBins) \\\n            .build()\n    \n    elif pALGORITHM == 'GB':\n        print('Gradient BOOSTING')\n        \n        from pyspark.ml.classification import GBTClassifier \n        \n        if CON_SCALER == True:\n            model = GBTClassifier(labelCol=\"label_\", featuresCol=\"features_scaled\", seed=12345)\n        else:\n            model = GBTClassifier(labelCol=\"label_\", featuresCol=\"features\", seed=12345)\n            \n        paramGrid = ParamGridBuilder() \\\n            .addGrid(model.maxIter, maxIter) \\\n            .addGrid(model.maxDepth, maxDepth) \\\n            .addGrid(model.minInstancesPerNode, minInstancesPerNode) \\\n            .addGrid(model.maxBins, maxBins) \\\n            .build()\n\n\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml import Pipeline\n    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n    \n    #separo Train y Test\n    # No entrenar con mas de 500k casos... \n    \n    (trainingData, testData) = train_undersampled_df.randomSplit([Training_Porcentaje, (1 - Training_Porcentaje)], seed=1234)\n    \n    print(\"TRAIN Shape: \" , trainingData.count(), ' - ', len(trainingData.columns))\n    \n        \n    # Numerical vars\n    numericCols = [c for c in trainingData.columns if c not in [CAMPO_CLAVE,'periodo', 'origin', 'label']]\n    \n    print(\"Num. numeric vars: \" , len(numericCols))\n    \n    \n    from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n    # Target\n    target_st = StringIndexer(inputCol=TARGET, outputCol='label_')\n    \n    # Variables\n    assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n    \n    scaler = StandardScaler(inputCol='features', outputCol='features_scaled', withStd=True, withMean=False)\n    \n    evaluator=BinaryClassificationEvaluator()\n    \n    crossval2 = CrossValidator(estimator=model,\n                              estimatorParamMaps=paramGrid,\n                              evaluator=evaluator,\n                              numFolds=3)  # use 3+ folds in practice\n    \n    if CON_SCALER == True:\n        print('Con Scaler !!!!!!!!!!!!!!!!!!!!!')\n        stages = [target_st, assembler, scaler, crossval2 ]\n    else:\n        \n        print('Sin Scaler !!!!!!!!!!!!!!!!!!!!!')\n        stages = [target_st, assembler, crossval2 ]\n    \n    pipeline = Pipeline(stages=stages)\n    \n    # Run cross-validation, and choose the best set of parameters.\n    cvModel2 = pipeline.fit(trainingData)\n    \n    testDataScore = cvModel2.transform(testData)\n    auc_cv = evaluator.evaluate(testDataScore, {evaluator.metricName: \"areaUnderROC\"})\n    print('*'*20)\n    print('auc 1 ', auc_cv)\n    \n    # El mejor modelo entrenado\n    bestModel = cvModel2.stages[-1].bestModel\n    \n    hyperparametros = bestModel.extractParamMap()\n    \n    ##############################################\n    # Enteno el mejor Modelo\n    ##############################################\n    \n    if pALGORITHM == 'RF':\n            \n        numTrees = bestModel.getOrDefault('numTrees')\n        maxDepth = bestModel.getOrDefault('maxDepth')\n        minInstancesPerNode = bestModel.getOrDefault('minInstancesPerNode')\n        maxBins = bestModel.getOrDefault('maxBins')\n        \n        paramGrid3 = ParamGridBuilder() \\\n            .addGrid(model.numTrees, [numTrees]) \\\n            .addGrid(model.maxDepth, [maxDepth]) \\\n            .addGrid(model.minInstancesPerNode, [minInstancesPerNode]) \\\n            .addGrid(model.maxBins, [maxBins]) \\\n            .build()\n        \n    elif pALGORITHM == 'GB':\n        \n        maxIter = bestModel.getOrDefault('maxIter')\n        maxDepth = bestModel.getOrDefault('maxDepth')\n        minInstancesPerNode = bestModel.getOrDefault('minInstancesPerNode')\n        maxBins = bestModel.getOrDefault('maxBins')\n        \n        paramGrid3 = ParamGridBuilder() \\\n            .addGrid(model.maxIter, [maxIter]) \\\n            .addGrid(model.maxDepth, [maxDepth]) \\\n            .addGrid(model.minInstancesPerNode, [minInstancesPerNode]) \\\n            .addGrid(model.maxBins, [maxBins]) \\\n            .build()\n           \n\n    crossval3 = CrossValidator(estimator=model,\n                              estimatorParamMaps=paramGrid3,\n                              evaluator=evaluator,\n                              numFolds=3)  # use 3+ folds in practice\n    \n    if CON_SCALER == True:\n        stages = [target_st, assembler, scaler, crossval3 ]\n    else:\n        stages = [target_st, assembler, crossval3 ]\n        \n    pipeline3 = Pipeline(stages=stages)\n    \n    # Run cross-validation, and choose the best set of parameters.\n    cvModel3 = pipeline3.fit(trainingData)\n   \n    ##########################################\n    # Variables Importantes\n    feat_imp = pd.DataFrame((cvModel3.stages[-1].bestModel.featureImportances.toArray()), index=numericCols).reset_index()\n    feat_imp.columns =['variable', 'importance']\n    \n    \n    SMALL_ALGO = Nombre_Modelo.lower()\n    BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    \n    \n    model_cvresults = spark.createDataFrame(\n        feat_imp, \n        [ \"variable\", \"importance\"]  \n    )\n    \n    model_cvresults = model_cvresults.withColumn(\"bin\", F.lit(BINARIO))\\\n                                    .withColumn(\"periodo\", F.lit(PERIODO))\\\n                                    .withColumn(\"algorithm\", F.lit(Nombre_Modelo)) \n    \n\n    try:\n        a = spark.sql(\"select count(1) from sdb_datamining.\" + modelo + '_feature_importance')\n        model_cvresults.write.mode('append').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_feature_importance')\n    except:\n        model_cvresults.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_feature_importance')\n    \n    \n    \n    #########################################################\n    from pyspark.sql.functions import udf\n    from pyspark.sql.types import FloatType\n\n    firstelement=udf(lambda v:float(v[1]),FloatType())\n\n    trainingDataScore = cvModel3.transform(trainingData)\n    trainingDataScore = trainingDataScore.withColumn('Prob1', firstelement('probability'))\n\n    testDataScore = cvModel3.transform(testData)\n    testDataScore = testDataScore.withColumn('Prob1', firstelement('probability'))\n    \n    auc_cv = evaluator.evaluate(testDataScore, {evaluator.metricName: \"areaUnderROC\"})\n    print('*'*20)\n    print('auc 2 ', auc_cv)\n    \n    print('*'*20)\n    print('*'*20)\n    \n    print(cvModel3.stages[-1].bestModel.extractParamMap())\n    ################################################\n    \n    df_values_lst = []\n    df_values_lst.append((PERIODO, Nombre_Modelo  , \"hyperparametros\", str(hyperparametros)))\n    df_values_lst.append((PERIODO, Nombre_Modelo , \"AUC_VALIDACION\", str(auc_cv)))\n\n    model_cvresults = spark.createDataFrame(\n        df_values_lst, \n        [\"periodo\", \"algorithm\", \"metric_desc\", \"metric_value\"]  \n    )\n    \n    \n    SMALL_ALGO = Nombre_Modelo.lower()\n    BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    \n    # Add bin column\n    model_cvresults = model_cvresults.withColumn(\"bin\", F.lit(BINARIO))\n\n    # Order columns\n    model_cvresults = model_cvresults.select(\"algorithm\", \"metric_desc\", \"metric_value\", \"periodo\", \"bin\")\n\n    # Cambiar esto con la tabla original\n    \n    try:\n        a = spark.sql(\"select count(1) from sdb_datamining.\" + modelo + '_metricas')\n        model_cvresults.write.mode('append').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_metricas')\n    except:\n        model_cvresults.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_metricas')\n    \n    ###############################################\n    # Sacar para grabar\n    \n    ### Save model\n    # Seleccionamos el mejor modelo y lo guardamos para compararlo con el otros modelos para luego elegir el modelo ganador\n    if 1 > 10:\n        SMALL_ALGO = Nombre_Modelo.lower()\n        BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    \n        cvModel3.write().overwrite().save(PATH + '/' + BINARIO + \".bin\")\n        \n    try:\n        CalcularDeciles(trainingDataScore.select(CAMPO_CLAVE, 'label', 'Prob1').toPandas(), testDataScore.select(CAMPO_CLAVE, 'label', 'Prob1').toPandas())\n    except:\n        print('Error Calcular Deciles')\n    return cvModel3","user":"u632820","dateUpdated":"2022-11-24T11:22:26-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228366923_-1254573993","id":"20221104-100421_1613358255","dateCreated":"2022-11-23T15:32:46-0300","dateStarted":"2022-11-24T11:22:26-0300","dateFinished":"2022-11-24T11:22:27-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"text":"%livy.pyspark\n\ndef TestingModeloSpark(pALGORITHM, testing_df, pModel_train, Nombre_Modelo):\n    import pandas as pd\n    \n    import pyspark.sql.functions as F\n    \n    ###################################################\n    # Scoreo\n    \n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml import Pipeline\n    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n    from pyspark.ml.classification import RandomForestClassifier\n        \n    evaluator=BinaryClassificationEvaluator()\n    \n    testDataScore_Val = pModel_train.transform(testing_df) \n    auc_cv = evaluator.evaluate(testDataScore_Val, {evaluator.metricName: \"areaUnderROC\"})\n    print('*'*20)\n    print('auc 2 ', auc_cv)\n    \n    ###################################################\n    # Grabo Resultados\n    \n    df_values_lst = []\n    df_values_lst.append((PERIODO, Nombre_Modelo , \"AUC_TESTEO\", str(auc_cv)))\n    \n    model_cvresults = spark.createDataFrame(df_values_lst, [\"periodo\", \"algorithm\", \"metric_desc\", \"metric_value\"]  )\n    SMALL_ALGO = Nombre_Modelo.lower()\n    BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    model_cvresults = model_cvresults.withColumn(\"bin\", F.lit(BINARIO))\n    model_cvresults = model_cvresults.select(\"algorithm\", \"metric_desc\", \"metric_value\", \"periodo\", \"bin\")\n    \n    model_cvresults.write.mode('append').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_metricas')","user":"u632820","dateUpdated":"2022-11-24T11:45:08-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228367121_310248565","id":"20221107-090927_701312181","dateCreated":"2022-11-23T15:32:47-0300","dateStarted":"2022-11-24T11:45:08-0300","dateFinished":"2022-11-24T11:45:09-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"text":"%livy.pyspark\n\n\ndef EntrenarModeloPandas(pALGORITHM, train_undersampled_df, Training_Porcentaje, param_test, Nombre_Modelo):\n#Training_Porcentaje = 0.1\n#pALGORITHM = 'XGB'\n\n#if 1 > 0:\n\n    import pandas as pd\n    \n    import pyspark.sql.functions as F\n    \n    \n     #separo Train y Test\n    # No entrenar con mas de 500k casos... \n    \n    (trainingData, testData) = train_undersampled_df.randomSplit([Training_Porcentaje, (1-Training_Porcentaje)], seed=1234)\n    \n    print(\"TRAIN Shape: \" , trainingData.count(), ' - ', len(trainingData.columns))\n    \n    \n    ####################################################\n    \n    import numpy as np\n    \n    df = trainingData.toPandas()\n    df['TGT'] = df['label'].astype(np.int)\n    try:\n        df.drop('label', axis=1, inplace=True)\n    except:\n        pass\n    \n    from sklearn.model_selection import train_test_split\n    \n    ####################################################\n    # Train y test\n    \n    X_train, X_test = train_test_split(df.copy(), test_size=0.3, random_state=42, stratify=df['TGT']);  \n    \n    \n    ###################################################\n    # Standarizar \n    ###################################################\n    \n    from sklearn import preprocessing\n    # Get column names first\n    \n    names_df = spark.sql(\"select * from sdb_datamining.\" + modelo + \"_variables where variable not in ( 'label' , '\" + CAMPO_CLAVE + \"') order by variable  \")\n    numericCols = names_df.select('variable').rdd.flatMap(lambda x: x).collect()\n    \n    \n    #numericCols = [c for c in df.columns if c not in [CAMPO_CLAVE,'periodo', 'origin', 'TGT', 'label']]\n    print('columnas ', len(numericCols))\n    \n    numerical_cols = numericCols\n    #numerical_cols = idx[(idx.str == False ) & (idx.col != 'TGT') & (idx.col != CAMPO_CLAVE)]['col']\n    names = numerical_cols\n    \n    X_train[numerical_cols] = X_train[numerical_cols].astype(np.float64)\n    X_test[numerical_cols] = X_test[numerical_cols].astype(np.float64)\n    \n    # Create the Scaler object\n    scaler = preprocessing.StandardScaler(copy=True)\n    \n    if CON_SCALER == True:\n        \n        print('Con Scaler !!!!!!!!!!!!!!!!!!!!!')\n        scaler.fit(X_train[names])\n        # Fit your data on the scaler object\n        scaled_est = scaler.transform(X_train[names])\n        scaled_est = pd.DataFrame(scaled_est, columns=names, index=X_train.index)\n        \n        X_train.drop(names, axis=1, inplace = True)\n        X_train2 = pd.concat((X_train, scaled_est), axis=1, sort=False)\n        \n        X_train2.head(1).T\n        # test\n        scaled_est_test = scaler.transform(X_test[names])\n        scaled_est_test = pd.DataFrame(scaled_est_test, columns=names, index=X_test.index)\n        X_test.drop(names, axis=1, inplace = True)\n        X_test2 = pd.concat((X_test, scaled_est_test), axis=1, sort=False)\n        \n        X_train2.shape\n        X_test2.shape\n        \n        X_train = X_train2.copy()\n        X_test = X_test2.copy()\n        \n    \n    from sklearn.metrics import roc_auc_score\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import StratifiedKFold\n    \n    \n    target_column = 'TGT'\n        \n    \n    print(numerical_cols)\n    \n    cross_val = StratifiedKFold(n_splits=10) \n        \n    if pALGORITHM == 'LGBM':\n        print('LGBM')\n        #########################################################\n        # Modelo 1\n        #########################################################\n        \n        \n        import lightgbm as lgb\n        \n        fit_params={\"early_stopping_rounds\":1000, \n                    \"eval_metric\" : 'auc', \n                    \"eval_set\" : [(X_test[numerical_cols],X_test[target_column])],\n                    'verbose': 100\n        }\n        \n        \n        \n        #This parameter defines the number of HP points to be tested\n        \n        #n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n        \n        clf = lgb.LGBMClassifier(random_state=314, silent=True, metric='None',  \n                                 #nfold=10, \n                                 n_jobs=4)\n        \n        from sklearn.model_selection import GridSearchCV\n        \n        \n        gs = RandomizedSearchCV( estimator=clf, param_distributions=param_test, \n                                    n_iter=100,\n                                    scoring='roc_auc',\n                                    cv=cross_val,\n                                    refit=True,\n                                    random_state=314,\n                                    verbose=True)\n        \n        gs.fit(X_train[numerical_cols],X_train[target_column], **fit_params)\n        \n        # principales variables\n        feat_imp = pd.Series(gs.best_estimator_.feature_importances_, index=X_train[numerical_cols].columns)\n        \n        ############################\n        # El mejor modelo\n        ############################\n        \n        opt_parameters = gs.best_estimator_.get_params()\n        \n        print(opt_parameters)\n        \n        #Configure from the HP optimisation\n        def learning_rate_010_decay_power_0995(current_iter):\n            base_learning_rate = 0.1\n            lr = base_learning_rate  * np.power(.995, current_iter)\n            return lr if lr > 1e-3 else 1e-3\n        #clf_final = lgb.LGBMClassifier(**gs.best_estimator_.get_params())\n        \n        #Configure locally from hardcoded values\n        clf_final = lgb.LGBMClassifier(**clf.get_params())\n        print(clf.get_params())\n        \n        \n        #set optimal parameters\n        clf_final.set_params(**opt_parameters)\n        \n        #Train the final model with learning rate decay\n        clf_final_train = clf_final.fit(X_train[ numerical_cols ], X_train[target_column],\n                                        **fit_params, \n                                        callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])\n        clf_final_train.best_score_\n        \n    \n    elif pALGORITHM == 'XGB':\n        print('XGBoost')\n        \n        \n        import xgboost as xgb\n\n             \n        xgb_model = xgb.XGBClassifier(objective='binary:logistic',\n                                    seed = 1234,\n                                    base_score = 0.5,\n                                    booster = 'gbtree',\n                                    gpu_id = -1,\n                                    importance_type = 'gain',\n                                    reg_alpha = 0.11,\n                                    scale_pos_weight = 1,\n                                    tree_method = 'exact',\n                                    min_child_weight=0.6,\n                                    colsample_bytree = 0.8,\n                                    subsample = 0.85)\n        \n        gs = RandomizedSearchCV( estimator=xgb_model, \n                                    param_distributions=param_test, \n                                    n_iter=100,\n                                    scoring='roc_auc',\n                                    cv=cross_val,\n                                    refit=True,\n                                    random_state=314,\n                                    verbose=True)\n        \n        gs.fit(X_train[numerical_cols],X_train[target_column])\n        \n        ############################\n        # El mejor modelo\n        ############################\n        \n        opt_parameters = gs.best_estimator_.get_params()\n        \n        print(opt_parameters)\n        \n        \n        xgb_model = xgb.XGBClassifier(objective='binary:logistic',\n                                    seed = 1234,\n                                    base_score = 0.5,\n                                    booster = 'gbtree',\n                                    gpu_id = -1,\n                                    importance_type = 'gain',\n                                    reg_alpha = 0.11,\n                                    scale_pos_weight = 1,\n                                    tree_method = 'exact',\n                                    min_child_weight=0.6,\n                                    colsample_bytree = 0.8,\n                                    subsample = 0.85,\n                                    gamma = gs.best_estimator_.get_params()['gamma'],\n                                    max_depth = gs.best_estimator_.get_params()['max_depth'],\n                                    n_estimators = gs.best_estimator_.get_params()['n_estimators'],\n                                    learning_rate = gs.best_estimator_.get_params()['learning_rate'],\n                                    \n                                    )\n                                    \n                                    \n                                    \n        #Train the final model with learning rate decay\n        clf_final_train = xgb_model.fit(X_train[ numerical_cols ], X_train[target_column] )\n\n    \n    ##########################################\n    # Variables Importantes\n    feat_imp = pd.DataFrame(clf_final_train.feature_importances_, index=X_train[numerical_cols].columns).reset_index()\n    feat_imp.columns =['variable', 'importance']\n    \n    \n    SMALL_ALGO = Nombre_Modelo.lower()\n    BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    \n    \n    model_cvresults = spark.createDataFrame(\n        feat_imp, \n        [ \"variable\", \"importance\"]  \n    )\n    \n    model_cvresults = model_cvresults.withColumn(\"bin\", F.lit(BINARIO))\\\n                                    .withColumn(\"periodo\", F.lit(PERIODO))\\\n                                    .withColumn(\"algorithm\", F.lit(Nombre_Modelo)) \n    \n    \n    \n\n    try:\n        a = spark.sql(\"select count(1) from sdb_datamining.\" + modelo + '_feature_importance')\n        model_cvresults.write.mode('append').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_feature_importance')\n    except:\n        model_cvresults.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_feature_importance')\n    \n    \n    ###############################################\n\n    # results..\n    \n    probabilities_train = clf_final_train.predict_proba(X_train[numerical_cols])\n    a = X_train[[target_column, CAMPO_CLAVE]].reset_index()\n    a.columns = ['idx1', 'label', CAMPO_CLAVE]\n    b = pd.DataFrame(probabilities_train[:,1], columns=['Prob1']).reset_index()\n    trainDataScore = pd.concat([a, b], axis=1)\n    \n\n\n    probabilities = clf_final_train.predict_proba(X_test[numerical_cols])\n    a = X_test[[target_column, CAMPO_CLAVE]].reset_index()\n    a.columns = ['idx1', 'label', CAMPO_CLAVE]\n    b = pd.DataFrame(probabilities[:,1], columns=['Prob1']).reset_index()\n    testDataScore = pd.concat([a, b], axis=1)\n    \n    \n    y_pred = clf_final_train.predict(X_test[numerical_cols])\n    \n    ##############################################\n    # ROC\n    \n    from sklearn.metrics import roc_auc_score, accuracy_score\n    import numpy as np\n    \n    a = pd.DataFrame(X_test[[target_column, CAMPO_CLAVE]], columns=['TGT', CAMPO_CLAVE])\n    a = a.reset_index()\n    b = pd.DataFrame(probabilities[:,1], columns=['Prob1'])\n    \n    result = pd.concat([a, b], axis=1)\n    \n    yPred = y_pred\n    yScore = result['Prob1']\n    yTest = result['TGT']\n    areaBajoCurvaRoc = roc_auc_score(yTest, yScore)\n    accuracy = accuracy_score(yTest, yPred)\n    \n    print('ROC: ', areaBajoCurvaRoc)\n    \n    auc_cv = areaBajoCurvaRoc\n    hyperparametros = opt_parameters\n    \n    ################################################\n    \n    df_values_lst = []\n    df_values_lst.append((PERIODO, Nombre_Modelo  , \"hyperparametros\", str(hyperparametros)))\n    df_values_lst.append((PERIODO, Nombre_Modelo , \"AUC_VALIDACION\", str(auc_cv)))\n\n    model_cvresults = spark.createDataFrame(\n        df_values_lst, \n        [\"periodo\", \"algorithm\", \"metric_desc\", \"metric_value\"]  \n    )\n    \n    \n    SMALL_ALGO = Nombre_Modelo.lower()\n    BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    \n    # Add bin column\n    model_cvresults = model_cvresults.withColumn(\"bin\", F.lit(BINARIO))\n\n    # Order columns\n    model_cvresults = model_cvresults.select(\"algorithm\", \"metric_desc\", \"metric_value\", \"periodo\", \"bin\")\n\n    # Cambiar esto con la tabla original\n    \n    try:\n        a = spark.sql(\"select count(1) from sdb_datamining.\" + modelo + '_metricas')\n        model_cvresults.write.mode('append').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_metricas')\n    except:\n        model_cvresults.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_metricas')\n    \n    \n    \n    ###############################################\n    # Sacar para grabar\n    \n   \n    ### Save model\n    # Seleccionamos el mejor modelo y lo guardamos para compararlo con el otros modelos para luego elegir el modelo ganador\n    if 1 > 10:\n        SMALL_ALGO = Nombre_Modelo.lower()\n        BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n    \n        scaler.write().overwrite().save(PATH + '/' + BINARIO + \"_scaler.bin\")\n        \n        clf_final_train.write().overwrite().save(PATH + '/' + BINARIO + \"_model.bin\")\n\n    try:\n        \n        CalcularDeciles(trainDataScore[[CAMPO_CLAVE, 'label', 'Prob1']], testDataScore[[CAMPO_CLAVE, 'label', 'Prob1']])\n    except:\n        print('Error Calcular Deciles')\n    return scaler, clf_final_train","user":"u632820","dateUpdated":"2022-11-24T02:19:18-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228367320_-2041395392","id":"20221107-114748_569157287","dateCreated":"2022-11-23T15:32:47-0300","dateStarted":"2022-11-24T02:19:19-0300","dateFinished":"2022-11-24T02:19:20-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"text":"%livy.pyspark\n\ndef TestingModeloPython(pALGORITHM, testing_df, pScaler_train, pModel_train, Nombre_Modelo ):\n \n#pALGORITHM = 'XGB'\n#pScaler_train = XGB_scaler_train\n#pModel_train = XGB_model_train\n\n# if 1 > 10:\n\n\n    import pyspark.sql.functions as F\n    import pandas as pd\n    import numpy as np\n\n    # Leo las variables de entrada al modelo\n    \n    names_df = spark.sql(\"select * from sdb_datamining.\" + modelo + \"_variables where  variable not in ( 'label' , '\" + CAMPO_CLAVE + \"')  order by variable  \")\n    names = names_df.select('variable').rdd.flatMap(lambda x: x).collect()\n    \n    \n    from sklearn import preprocessing\n    \n    ###################################################\n    \n    X_test = testing_df.select(*names).toPandas()\n    \n    \n    print(X_test.shape)\n    \n    ###################################################\n    \n    print(len(names))\n    \n    X_test[names] = X_test[names].astype(np.float64)\n    \n    if CON_SCALER == True:\n        scaled_est_test = pScaler_train.transform(X_test[names])\n        scaled_est_test = pd.DataFrame(scaled_est_test, columns=names, index=X_test.index)\n        X_test = scaled_est_test.copy()\n    \n    \n    if pALGORITHM == 'LGBM':\n        print('LGBM')       \n        import lightgbm as lgb\n\n    elif pALGORITHM == 'XGB':\n        print('XGBoost')\n        import xgboost as xgb\n    \n    y_test = testing_df.select('label').toPandas()\n\n    probabilities       = pModel_train.predict_proba(X_test[names])\n    y_pred              = pModel_train.predict(X_test[names])\n\n    ##############################################\n    # ROC\n    \n    from sklearn.metrics import roc_auc_score, accuracy_score\n    import numpy as np\n    \n    a = pd.DataFrame(y_test[['label']], columns=['label'])\n    a = a.reset_index()\n    b = pd.DataFrame(probabilities[:,1], columns=['Prob1'])\n    \n    result = pd.concat([a, b], axis=1)\n    \n    yPred = y_pred\n    yScore = result['Prob1']\n    yTest = result['label']\n    auc_cv = roc_auc_score(yTest, yScore)\n    accuracy = accuracy_score(yTest, yPred)\n    \n    print('ROC : ', auc_cv)\n    \n    ###################################################\n    # Grabo Resultados\n    \n    df_values_lst = []\n    df_values_lst.append((PERIODO, Nombre_Modelo  , \"AUC_TESTEO\", str(auc_cv)))\n    \n    model_cvresults = spark.createDataFrame(df_values_lst, [\"periodo\", \"algorithm\", \"metric_desc\", \"metric_value\"]  )\n\n    SMALL_ALGO = Nombre_Modelo.lower()\n    BINARIO = f\"{PERIODO}_challenger_{SMALL_ALGO}\"\n\n    model_cvresults = model_cvresults.withColumn(\"bin\", F.lit(BINARIO))\n    model_cvresults = model_cvresults.select(\"algorithm\", \"metric_desc\", \"metric_value\", \"periodo\", \"bin\")\n\n    model_cvresults.write.mode('append').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_metricas')\n    \n    ","user":"u632820","dateUpdated":"2022-11-24T11:51:13-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669300432950_177149723","id":"20221124-113352_622366410","dateCreated":"2022-11-24T11:33:52-0300","dateStarted":"2022-11-24T11:55:33-0300","dateFinished":"2022-11-24T11:55:34-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%livy.pyspark\r\n\r\ndef CalcularDeciles(pTrain, pTest):\r\n    \r\n    import numpy as np\r\n\r\n    \r\n    ###############################################\r\n    print('Training')\r\n    result = pTrain\r\n    \r\n    result['porc'] = result['Prob1'].rank(pct=True) * 100\r\n    \r\n    \r\n    result.loc[result['porc'].between(0, 10, inclusive=False), 'decil'] = 10\r\n    result.loc[result['porc'].between(10, 20, inclusive=True), 'decil'] = 9\r\n    result.loc[result['porc'].between(20, 30, inclusive=False), 'decil'] = 8\r\n    result.loc[result['porc'].between(30, 40, inclusive=True), 'decil'] = 7\r\n    result.loc[result['porc'].between(40, 50, inclusive=False), 'decil'] = 6\r\n    result.loc[result['porc'].between(50, 60, inclusive=True), 'decil'] = 5\r\n    result.loc[result['porc'].between(60, 70, inclusive=False), 'decil'] = 4\r\n    result.loc[result['porc'].between(70, 80, inclusive=True), 'decil'] = 3\r\n    result.loc[result['porc'].between(80, 90, inclusive=False), 'decil'] = 2\r\n    result.loc[result['porc'].between(90, 101, inclusive=True), 'decil'] = 1\r\n    \r\n    print(result.decil.value_counts())\r\n    print(result[result.label == 1].decil.value_counts())\r\n    \r\n    a = result.groupby('decil')['Prob1'].agg(min)\r\n    print(a)\r\n    \r\n    import pandas as pd\r\n    \r\n    deciles = pd.DataFrame(result.groupby('decil')['Prob1'].min().reset_index())\r\n    deciles.columns = ['decil', 'cota']\r\n\r\n    ##############################################\r\n    \r\n    result = pTest\r\n    print('*'*20)\r\n    print('Testing')\r\n    result['decil'] = np.where(result.Prob1 >= deciles[deciles.decil == 1]['cota'][0]                                  , 1,  \r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 2]['cota'][1]) & (result.Prob1 < deciles[deciles.decil == 1]['cota'][0] ), 2,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 3]['cota'][2]) & (result.Prob1 < deciles[deciles.decil == 2]['cota'][1] ) , 3,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 4]['cota'][3] ) & (result.Prob1 < deciles[deciles.decil == 3]['cota'][2]), 4,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 5]['cota'][4] ) & (result.Prob1 < deciles[deciles.decil == 4]['cota'][3]), 5,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 6]['cota'][5] ) & (result.Prob1 < deciles[deciles.decil == 5]['cota'][4]), 6,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 7]['cota'][6] ) & (result.Prob1 < deciles[deciles.decil == 6]['cota'][5]) , 7,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 8]['cota'][7] ) & (result.Prob1 < deciles[deciles.decil == 7]['cota'][6]), 8,\r\n                            np.where((result.Prob1 >=  deciles[deciles.decil == 9]['cota'][8] ) & (result.Prob1 < deciles[deciles.decil == 8]['cota'][7]), 9,\r\n                            10)))))))))\r\n\r\n    print(result.decil.value_counts())\r\n    print(result[result.label == 1].decil.value_counts())","user":"u632820","dateUpdated":"2022-11-24T02:19:20-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669229214899_-1544932786","id":"20221123-154654_1475396306","dateCreated":"2022-11-23T15:46:54-0300","dateStarted":"2022-11-24T02:19:20-0300","dateFinished":"2022-11-24T02:19:21-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"text":"%livy.pyspark\n\ndef CrearTablas():\n    \n    import datetime;\n    ct = datetime.datetime.now()\n    print(\"Crear Tablas:-\", ct)\n    \n    ####################################################################\n    # 1. Crear tabla para Training\n    # 1.1. Leer ABT + TGT\n    ####################################################################\n    \n    from pyspark.sql import functions as F\n    \n    pABT    =   \" SELECT a.\" + CAMPO_CLAVE + \", \" + ABT_VARIABLES + \" , coalesce(\" + TGT_VARIABLES + \"\"\", 0) as label   \n                  FROM \"\"\" + ABT_TABLA +  \"\"\" a \"\"\" +  \"\"\" \n                        left join \"\"\" + TGT_TABLA + \" b  on a.\" + CAMPO_CLAVE + \" = b.\" + CAMPO_CLAVE + \"\"\"\n                                                          AND a.periodo = b.periodo \n                  WHERE   a.periodo IN (\"\"\" + str(PERIODO_TRAIN1) + \" , \" +  str(PERIODO_TRAIN2)  + \" , \" +str( PERIODO_TRAIN3)  + \" , \" + str(PERIODO_TRAIN4)  + \" , \" + str(PERIODO_TRAIN5)  + \" , \" + str(PERIODO_TRAIN6) + \")\"\n    \n    # cargamos las variables de la abt que se uso en el modelo productivo y el target con los periodos de training para realizar el entrenamiento \n    train_undersampled_df = spark.sql(pABT).na.fill(-999)\n    \n    train_undersampled_df.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_0')\n    \n    ####################################################################\n    # 1.2. Balanceo y Particiones\n    ####################################################################\n    \n    train_undersampled_df = spark.sql(\"select * from sdb_datamining.\" +  modelo + '_0')\n    print('TABLA ORIGINAL: ', train_undersampled_df.count())\n    \n    if BALANCEAR_TARGET  == True:\n        train_undersampled_df = BalancearABT(train_undersampled_df, TGT_BALENCEO)\n    \n    train_undersampled_df = ControlParticiones(train_undersampled_df, PARTICIONES) \n    \n    train_undersampled_df.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_1')\n    \n    ####################################################################\n    # 1.3. Corregir Numeros, Eliminar Correlaciones y Particiones\n    ####################################################################\n    \n    train_undersampled_df = spark.sql(\"select * from sdb_datamining.\" +  modelo + '_1')\n    \n    if CASTEAR_BIGINT == True:\n        train_undersampled_df = CastBigInt(train_undersampled_df)\n        \n    if REDONDEAR_DECIMALES == True:\n        train_undersampled_df = RedondearDecimales(train_undersampled_df, DECIMALES_VARIABLES_NUMERICAS)\n        \n    if ELIMINAR_CORRELACIONES == True:\n        train_undersampled_df = EliminarCorrelaciones(train_undersampled_df, COTA_CORRELACIONES)\n    \n    \n    train_undersampled_df = ControlParticiones(train_undersampled_df, PARTICIONES) \n    train_undersampled_df.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' +  modelo + '_2' )\n\n    ####################################################################\n    # 1.4. Grabo las variables que van a entrar al modelo\n    ####################################################################\n    \n    # Grabar las variables.....\n    import pandas as pd\n    columns = ['variable']\n    \n    variable = pd.DataFrame(train_undersampled_df.columns)\n    variable.columns = ['variable']\n    \n    variable = spark.createDataFrame(\n            variable, \n            [\"variable\"]  \n        )\n        \n    variable.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_variables' )\n\n    ####################################################################\n    ## 2. Cargar ABT de Testing\n    ####################################################################\n    \n    if TIENE_TESTING == True:\n        \n        pABT    =   \" SELECT a.periodo , a.\" + CAMPO_CLAVE + \", \" + ABT_VARIABLES + \" , coalesce(\" + TGT_VARIABLES + \"\"\", 0) as label   \n                      FROM \"\"\" + ABT_TABLA +  \"\"\" a \"\"\" +  \"\"\" \n                            left join \"\"\" + TGT_TABLA + \" b  on a.\" + CAMPO_CLAVE + \" = b.\" + CAMPO_CLAVE + \"\"\"\n                                                              AND a.periodo = b.periodo \n                      WHERE   a.periodo IN (\"\"\" + str(PERIODO_TEST1) + \" , \" +  str(PERIODO_TEST2)  + \" , \" +str( PERIODO_TEST3) + \")\"\n        \n        test_undersampled_df = spark.sql(pABT)\n        test_undersampled_df = spark.sql(pABT).na.fill(-999)\n        \n        print('TABLA PREDICCIONES: ', test_undersampled_df.count())\n        \n        #############################################################################\n        # estos pasos tienen que ser los mismos que los realizados en la ABT de Training\n        \n        if CASTEAR_BIGINT == True:\n            test_undersampled_df = CastBigInt(test_undersampled_df)\n            \n        if REDONDEAR_DECIMALES == True:\n            test_undersampled_df = RedondearDecimales(test_undersampled_df, DECIMALES_VARIABLES_NUMERICAS)\n        \n        \n        test_undersampled_df = ControlParticiones(test_undersampled_df, PARTICIONES) \n        test_undersampled_df.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' +  modelo + '_testing' )\n        ","user":"u632820","dateUpdated":"2022-11-24T12:17:08-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669230385493_1421713347","id":"20221123-160625_1619373798","dateCreated":"2022-11-23T16:06:25-0300","dateStarted":"2022-11-24T12:17:08-0300","dateFinished":"2022-11-24T12:17:09-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"%livy.pyspark\n\ndef EntrenarModelos(Corrida):\n    import datetime;\n    ct = datetime.datetime.now()\n\n    print(\"Entrenamiento:-\", ct)\n    \n    train_undersampled_df = spark.sql(\"select * from sdb_datamining.\" +  modelo  + '_2')\n    \n    if TIENE_TESTING == True:\n        testing_df_m1 = spark.sql(\"select * from sdb_datamining.\" +  modelo + '_testing where periodo = ' + str(PERIODO_TEST1) )\n        testing_df_m2 = spark.sql(\"select * from sdb_datamining.\" +  modelo + '_testing where periodo = ' + str(PERIODO_TEST2) )\n        testing_df_m3 = spark.sql(\"select * from sdb_datamining.\" +  modelo + '_testing where periodo = ' + str(PERIODO_TEST3) )\n\n\n    #############################################################################    \n    # 3. Entreno el Modelo        \n    #############################################################################\n    \n    # 3.1. Random Forest\n            \n    try:\n        if CORRER_RF == True:\n            print ('*'*30)\n            print (' RANDOM FOREST ')\n            Nombre_Modelo = 'RF_' + str(Corrida)\n            RF_Model_train = EntrenarModeloSpark('RF', train_undersampled_df, PORCENTAJE_TRAINING, RF_param_test, Nombre_Modelo  ) \n            \n            if TIENE_TESTING == True:        \n                print( 'Testing ')\n                TestingModeloSpark('RF', testing_df_m1, RF_Model_train, Nombre_Modelo + ' TESTING MES1')\n                TestingModeloSpark('RF', testing_df_m2, RF_Model_train, Nombre_Modelo + ' TESTING MES2')\n                TestingModeloSpark('RF', testing_df_m3, RF_Model_train, Nombre_Modelo + ' TESTING MES3')\n    except:\n        print('Error Random Forest')\n        \n        \n    # 3.2. Gradient Boosting \n    try:\n        if CORRER_GB  == True:\n            print ('*'*30)\n            print (' GRADIENT BOOSTING ')\n    \n            Nombre_Modelo = 'GB_' + str(Corrida)             \n            GB_Model_train = EntrenarModeloSpark('GB', train_undersampled_df, PORCENTAJE_TRAINING, GB_param_test, Nombre_Modelo) \n            \n            if TIENE_TESTING == True:        \n                print( 'Testing ')\n                TestingModeloSpark('GB', testing_df_m1, GB_Model_train, Nombre_Modelo + ' TESTING MES1')\n                TestingModeloSpark('GB', testing_df_m2, GB_Model_train, Nombre_Modelo + ' TESTING MES2')\n                TestingModeloSpark('GB', testing_df_m3, GB_Model_train, Nombre_Modelo + ' TESTING MES3')\n    except:\n        print('Error Gradient Boosting')\n    \n    \n    ## 3.3. LIGHTGBM Pandas\n    try:    \n        if CORRER_LGBM  == True:\n            \n            print ('*'*30)\n            print (' LIGHTGBM ')\n            \n            Nombre_Modelo = 'LGBM_' + str(Corrida)\n            LGBM_scaler_train, LGBM_model_train = EntrenarModeloPandas('LGBM', train_undersampled_df, PORCENTAJE_TRAINING, LGBM_param_test, Nombre_Modelo)\n            \n            if TIENE_TESTING == True:        \n                print( 'Testing ')\n                TestingModeloPython('LGBM', testing_df_m1, LGBM_scaler_train, LGBM_model_train, Nombre_Modelo + ' TESTING MES1')\n                TestingModeloPython('LGBM', testing_df_m2, LGBM_scaler_train, LGBM_model_train, Nombre_Modelo + ' TESTING MES2')\n                TestingModeloPython('LGBM', testing_df_m3, LGBM_scaler_train, LGBM_model_train, Nombre_Modelo + ' TESTING MES3')\n    except:\n        print('Error Lightgbm')\n    \n    \n    try:\n        # 3.4. XGBoost Pandas\n        if CORRER_XGB == True:\n            print ('*'*30)\n            print (' XGB ')\n            \n            Nombre_Modelo = 'XGB_' + str(Corrida)\n            XGB_scaler_train, XGB_model_train = EntrenarModeloPandas('XGB', train_undersampled_df, PORCENTAJE_TRAINING, XGB_param_test, Nombre_Modelo)\n            \n            if TIENE_TESTING == True:        \n                print( 'Testing ')\n                TestingModeloPython('XGB', testing_df_m1, XGB_scaler_train, XGB_model_train, Nombre_Modelo + ' TESTING MES1')\n                TestingModeloPython('XGB', testing_df_m2, XGB_scaler_train, XGB_model_train, Nombre_Modelo + ' TESTING MES2')\n                TestingModeloPython('XGB', testing_df_m3, XGB_scaler_train, XGB_model_train, Nombre_Modelo + ' TESTING MES3')\n         \n    except:\n        print('Error XGBoost')\n          \n    try:\n            \n        # 3.5. Modelo Productivo\n        \n        if CORRER_PRODUCTIVO == True:\n            print ('*'*30)\n            print (' MODELO PRODUCTIVO ')\n            \n            if MODELO_PRODUCTIVO == 'RF':\n            \n                Nombre_Modelo = 'RF_PRODUCTIVO' + str(Corrida)           \n                Productivo_Model_train = EntrenarModeloSpark('RF', train_undersampled_df, PORCENTAJE_TRAINING, MODELO_PRODUCTIVO_param_test, Nombre_Modelo) \n            \n            elif MODELO_PRODUCTIVO == 'GB':\n                \n                Nombre_Modelo = 'GB_PRODUCTIVO'   + str(Corrida)                      \n                Productivo_Model_train = EntrenarModeloSpark('GB', train_undersampled_df, PORCENTAJE_TRAINING, MODELO_PRODUCTIVO_param_test, Nombre_Modelo) \n            \n            elif MODELO_PRODUCTIVO == 'LGBM':\n                    \n                Nombre_Modelo = 'LGBM_PRODUCTIVO' + str(Corrida)           \n                Productivo_scaler_train, Productivo__model_train = EntrenarModeloPandas('LGBM', train_undersampled_df, PORCENTAJE_TRAINING, MODELO_PRODUCTIVO_param_test, Nombre_Modelo)\n                \n            elif MODELO_PRODUCTIVO == 'XGB':\n                    \n                Nombre_Modelo = 'XGB_PRODUCTIVO' + str(Corrida)           \n                Productivo_scaler_train, Productivo__model_train, XGB_trainingData_Score, XGB_testingData_Score = EntrenarModeloPandas('XGB', train_undersampled_df, PORCENTAJE_TRAINING, MODELO_PRODUCTIVO_param_test, Nombre_Modelo)\n                \n                CalcularDeciles(XGB_trainingData_Score, XGB_testingData_Score)\n                \n            if TIENE_TESTING == True:        \n                print( 'Testing ')\n                if MODELO_PRODUCTIVO == 'RF' or MODELO_PRODUCTIVO == 'GB':\n                      \n                    TestingModeloSpark(MODELO_PRODUCTIVO, testing_df_m1, Productivo_Model_train, Nombre_Modelo + ' TESTING MES1')\n                    TestingModeloSpark(MODELO_PRODUCTIVO, testing_df_m2, Productivo_Model_train, Nombre_Modelo + ' TESTING MES2')\n                    TestingModeloSpark(MODELO_PRODUCTIVO, testing_df_m3, Productivo_Model_train, Nombre_Modelo + ' TESTING MES3')\n                        \n                elif MODELO_PRODUCTIVO == 'XGB' or MODELO_PRODUCTIVO == 'LGBM':\n                    TestingModeloPython(MODELO_PRODUCTIVO, testing_df_m1, Productivo_scaler_train, Productivo__model_train, Nombre_Modelo + ' TESTING MES1')\n                    TestingModeloPython(MODELO_PRODUCTIVO, testing_df_m2, Productivo_scaler_train, Productivo__model_train, Nombre_Modelo + ' TESTING MES2')\n                    TestingModeloPython(MODELO_PRODUCTIVO, testing_df_m3, Productivo_scaler_train, Productivo__model_train, Nombre_Modelo + ' TESTING MES3')\n    except:\n        print('Error Modelo Productivo ')\n    \n    ct = datetime.datetime.now()\n    print(\"Entrenamiento Fin:-\", ct)\n    \n    \n    #############################################################################    \n    # 4. Mejores Variables de Todos los modelos\n    #############################################################################\n    \n    a = spark.sql(\"\"\"select variable, sum(rownum ) as rownum \n                     from (select ROW_NUMBER() OVER(PARTITION BY algorithm ORDER BY importance DESC) AS rownum, * \n                            from sdb_datamining.\"\"\" + modelo + \"\"\"_feature_importance) \n                    group by variable\n                    order by 2 \"\"\")\n                \n    a.write.mode('overwrite').format('parquet').saveAsTable('sdb_datamining.' + modelo + '_feature_importance_rank')","user":"u632820","dateUpdated":"2022-11-24T13:18:39-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669230592524_743456688","id":"20221123-160952_2133282021","dateCreated":"2022-11-23T16:09:52-0300","dateStarted":"2022-11-24T13:18:39-0300","dateFinished":"2022-11-24T13:18:40-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%livy.pyspark\n\ndef MejorModeloEntrenado():\n    #############################################################################    \n    # 5. Elijo el Mejor modelo entrenado\n    #############################################################################\n        \n    # Busco quien es el mejor en Testing\n    \n    meses_testing = spark.sql(\"\"\"\n        select  replace(replace(replace(A.ALGORITHM, 'MES1', ''), 'MES2', ''), 'MES3', '') AS ALGORITHM,\n                SUM(  case when mes1.ALGORITHM = a.ALGORITHM then 1 else 0 end\n                    + case when mes2.ALGORITHM = a.ALGORITHM then 1 else 0 end\n                    + case when mes3.ALGORITHM = a.ALGORITHM then 1 else 0 end ) as meses_ganadores\n                    \n                    \n        FROM sdb_datamining.\"\"\" + modelo + \"\"\"_metricas  A\n            left join (SELECT MAX(ALGORITHM)  AS ALGORITHM /* PONGO UN MAX POR SI EMPATAN QUE SE QUEDE CON UNO */\n                        FROM sdb_datamining.\"\"\" + modelo + \"\"\"_metricas  A,\n                                (select  max(metric_value) AS metric_value\n                                from sdb_datamining.\"\"\" + modelo + \"\"\"_metricas \n                                where algorithm like '% TESTING MES1'\n                                AND   metric_desc = 'AUC_TESTEO' ) B\n                        WHERE   A.metric_value = B.metric_value\n                                ) mes1  on a.ALGORITHM = mes1.ALGORITHM \n            \n            left join (SELECT MAX(ALGORITHM)  AS ALGORITHM /* PONGO UN MAX POR SI EMPATAN QUE SE QUEDE CON UNO */\n                        FROM sdb_datamining.\"\"\" + modelo + \"\"\"_metricas  A,\n                                (select  max(metric_value) AS metric_value\n                                from sdb_datamining.\"\"\" + modelo + \"\"\"_metricas \n                                where algorithm like '% TESTING MES2'\n                                AND   metric_desc = 'AUC_TESTEO' ) B\n                        WHERE   A.metric_value = B.metric_value\n                                ) mes2  on a.ALGORITHM = mes2.ALGORITHM \n                        \n            left join (SELECT MAX(ALGORITHM)  AS ALGORITHM /* PONGO UN MAX POR SI EMPATAN QUE SE QUEDE CON UNO */\n                        FROM sdb_datamining.\"\"\" + modelo + \"\"\"_metricas  A,\n                                (select  max(metric_value) AS metric_value\n                                from sdb_datamining.\"\"\" + modelo + \"\"\"_metricas \n                                where algorithm like '% TESTING MES3'\n                                AND   metric_desc = 'AUC_TESTEO' ) B\n                        WHERE   A.metric_value = B.metric_value\n                                ) mes3  on a.ALGORITHM = mes3.ALGORITHM \n            WHERE A.ALGORITHM LIKE '%TESTING MES%'\n            GROUP BY replace(replace(replace(A.ALGORITHM, 'MES1', ''), 'MES2', ''), 'MES3', '')                                 \n        \"\"\")\n    \n    print('*'*20)\n    print('mejor modelo en Testing')\n    meses_testing.show()\n    \n    meses_testing.createOrReplaceTempView(\"meses_testing\")\n    \n    \n    modelo_ganador_testing = spark.sql(\"\"\"\n        select  a.*\n        FROM sdb_datamining.\"\"\" + modelo + \"\"\"_metricas  A,\n                    (SELECT MAX(ALGORITHM) AS ALGORITHM\n                    FROM \n                        (select replace(replace(replace(A.ALGORITHM, 'MES1', ''), 'MES2', ''), 'MES3', '') AS ALGORITHM\n                        from   meses_TESTING A,\n                                (SELECT MAX(MESES_GANADORES) AS MESES_GANADORES\n                                FROM MESES_TESTING\n                                ) B\n                        WHERE  A.MESES_GANADORES = B.MESES_GANADORES\n                        )\n                    ) B\n            WHERE A.ALGORITHM LIKE '%TESTING MES%'\n            AND   replace(replace(replace(A.ALGORITHM, 'MES1', ''), 'MES2', ''), 'MES3', '') = B.ALGORITHM\n            \n        \"\"\")\n    \n    modelo_ganador_testing.show()\n    \n    modelo_ganador_testing.createOrReplaceTempView(\"modelo_ganador_testing\")\n    \n    \n    meses_ganadores_vs_produccion = spark.sql(\"\"\"\n        SELECT  SUM(MESES_GANADORES) AS MESES_GANADORES\n        FROM (\n              SELECT  A.*,\n                        B.metric_value,\n                        CASE WHEN B.metric_value - A.AUC_PROD > 0.02 THEN 1 ELSE 0 END AS MESES_GANADORES\n                from \n                        (SELECT  modelo,\n                                substr(cast(fecha AS STRING),1,6) AS periodo, \n                                sum(suma_area) AS auc_prod    \n                        FROM    data_lake_analytics.indicadores_performance  \n                        WHERE   substr(cast(fecha AS STRING),1,6) IN ( \"\"\" + str(PERIODO_TEST1) + ',' + str(PERIODO_TEST2) + ',' + str(PERIODO_TEST3) + \"\"\")\n                        AND     modelo = '\"\"\" + modelo + \"\"\"'\n                        AND     tipo = 'PERFORMANCE'\n                        group by modelo, substr(cast(fecha AS STRING),1,6) ) A\n                        \n                        LEFT JOIN modelo_ganador_testing B ON A.PERIODO = case when b.algorithm like '% TESTING MES1' then \"\"\" + str(PERIODO_TEST1) + \"\"\"\n                                                                                when b.algorithm like '% TESTING MES2' then \"\"\" + str(PERIODO_TEST2) + \"\"\"\n                                                                                when b.algorithm like '% TESTING MES3' then \"\"\" + str(PERIODO_TEST3) + \"\"\"\n                                                                        end           \n             ) sa\n        \"\"\").toPandas()['MESES_GANADORES'][0]\n        \n    print('MESES QUE EL NUEVO MODELO LE GANA AL PRODUCTIVO: ', meses_ganadores_vs_produccion)","user":"u632820","dateUpdated":"2022-11-25T16:46:31-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_91155<br/>Spark WebUI: <a href=\"null\">null</a>"}]},"apps":[],"jobName":"paragraph_1669230649769_240915996","id":"20221123-161049_594131705","dateCreated":"2022-11-23T16:10:49-0300","dateStarted":"2022-11-25T16:46:31-0300","dateFinished":"2022-11-25T16:46:32-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"%livy.pyspark\nMejorModeloEntrenado()","user":"u632820","dateUpdated":"2022-11-25T16:46:35-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"********************\nmejor modelo en Testing\n+--------------------+---------------+\n|           ALGORITHM|meses_ganadores|\n+--------------------+---------------+\n|RF_PRODUCTIVO TES...|              0|\n|       RF_2 TESTING |              0|\n|      XGB_2 TESTING |              0|\n|     LGBM_1 TESTING |              0|\n|      XGB_1 TESTING |              0|\n|RF_PRODUCTIVO2 TE...|              0|\n|       RF_1 TESTING |              0|\n|       GB_1 TESTING |              1|\n|       GB_2 TESTING |              2|\n|     LGBM_2 TESTING |              0|\n+--------------------+---------------+\n\n+-----------------+-----------+------------------+-------+--------------------+\n|        algorithm|metric_desc|      metric_value|periodo|                 bin|\n+-----------------+-----------+------------------+-------+--------------------+\n|GB_2 TESTING MES3| AUC_TESTEO|0.6009550245227493| 202211|202211_challenger...|\n|GB_2 TESTING MES2| AUC_TESTEO|0.6073618586389412| 202211|202211_challenger...|\n|GB_2 TESTING MES1| AUC_TESTEO| 0.607391521745446| 202211|202211_challenger...|\n+-----------------+-----------+------------------+-------+--------------------+\n\nMESES QUE EL NUEVO MODELO LE GANA AL PRODUCTIVO:  3"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_91155<br/>Spark WebUI: <a href=\"null\">null</a>"}]},"apps":[],"jobName":"paragraph_1669305419373_200031108","id":"20221124-125659_1274195047","dateCreated":"2022-11-24T12:56:59-0300","dateStarted":"2022-11-25T16:46:35-0300","dateFinished":"2022-11-25T16:47:54-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%livy.pyspark\n\ndef EjecutarChallenger():\n#if 1 > 0:\n    import datetime;\n    ct = datetime.datetime.now()\n    print(\"Challenger Inicio:-\", ct)\n    \n    BorrarTablasTemporales()\n    \n    CrearTablas() \n    \n    ct = datetime.datetime.now()\n\n    EntrenarModelos(1)\n\n    MejorModeloEntrenado()\n    \n    ct = datetime.datetime.now()\n    print(\"Challenger Fin:-\", ct)","user":"u632820","dateUpdated":"2022-11-24T13:19:11-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669228368248_-37723786","id":"20221107-151913_958902940","dateCreated":"2022-11-23T15:32:48-0300","dateStarted":"2022-11-24T02:19:25-0300","dateFinished":"2022-11-24T02:19:26-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"text":"%md\n\n# PRUEBA!!!","user":"u632820","dateUpdated":"2022-11-24T02:19:26-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>PRUEBA!!!</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1669230981903_1120193952","id":"20221123-161621_470042427","dateCreated":"2022-11-23T16:16:21-0300","dateStarted":"2022-11-24T02:19:27-0300","dateFinished":"2022-11-24T02:19:27-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%livy.pyspark\n\nPERIODO = 202211 # periodo de corrida\n\nPERIODO_TRAIN1 = 202108\nPERIODO_TRAIN2 = 202109\nPERIODO_TRAIN3 = 202110\nPERIODO_TRAIN4 = 202111\nPERIODO_TRAIN5 = 202112\nPERIODO_TRAIN6 = 202201\n\nPERIODO_TEST1 = 202202\nPERIODO_TEST2 = 202203\nPERIODO_TEST3 = 202204  # 6 meses para atras desde el periodo que se corre el proceso o desde el periodo que esta disponible el target","user":"u632820","dateUpdated":"2022-11-25T16:46:27-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_91155<br/>Spark WebUI: <a href=\"null\">null</a>"}]},"apps":[],"jobName":"paragraph_1669230981074_157558303","id":"20221123-161621_1874949051","dateCreated":"2022-11-23T16:16:21-0300","dateStarted":"2022-11-25T16:46:27-0300","dateFinished":"2022-11-25T16:46:29-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"%livy.pyspark\r\n\r\n\r\nBALANCEAR_TARGET = True \r\nELIMINAR_CORRELACIONES = True\r\nCASTEAR_BIGINT = True\r\nREDONDEAR_DECIMALES = True\r\nCON_SCALER = True\r\n\r\nTIENE_TESTING = True  # Tiene Periodo de Testing\r\n\r\nCORRER_RF = True\r\nCORRER_GB = True\r\nCORRER_LGBM = True\r\nCORRER_XGB = True\r\nCORRER_PRODUCTIVO = True\r\n\r\n\r\nCAMPO_CLAVE = 'id_suscripcion'\r\n\r\n# Variables para que corra esta muestra en zeppelin\r\n\r\nTARGET= 'label'\r\nmodelo = \"CHURN_COMBO_FLOW\"\r\n\r\n\r\nPATH = \"/adv/modelos/prueba_challenger\"\r\n\r\n# No poner id_suscripcion ni periodo\r\nABT_VARIABLES = \"\"\" \r\n        sum_kb_up_maniana_m0, \r\n        tend_tot_kb_up_m3, zt_tel_m0, max_dur_cumsum_cutv_m0, avg_kb_up_noche_m0, countacum_pagos_electronico_u6m, stddev_zt_catv_u3m, \r\n        pct_change_kb_up_total_m3, pct_change_zt_cmdm_m1, tend_vod_time_m6, tend_kb_down_tarde_m3, cat_gam_heavy_user_ohe, tend_kb_up_weekend_m6, tend_tot_kb_down_m6, \r\n        avg_zt_cli_viv_u6m, diff_zt_tel_cli_m1, sum_acum_tot_prodpremium_u6m, pct_change_kb_up_total_m1, avg_rxpower_down_noche_m0, pct_change_zt_catv_m3, \r\n        sumacum_programas_cutv_u6m, pct_change_zt_cm_cli_m1, count_tot_pagos_centanilla_m0, sum_kb_up_tarde_m0, diff_zt_tel_m1, avg_kpi_mer_noobs_u3m, sum_vod_time_m0, \r\n        tend_zt_cli_viv_m6, pct_change_ah3_d_sem_live_m1, sum_acum_tot_tipo_prod_flow_cerrado_danio_u3m, zt_kms_m0, pct_change_kb_up_tarde_m3, \r\n        avg_tot_tipo_prod_cm_abierto_danio_u3m, sumacum_dias_cutv_u3m, sum_hbo_premium_time_live_m0, pct_change_phone_time_live_m1, pct_change_zt_tel_cli_m3, \r\n        avg_kb_up_weekend_u6m, pct_change_ah1_d_fin_live_m1, pct_change_kb_up_week_m1, avg_zt_cmdm_u6m, pct_change_ah2_d_sem_live_m1, avg_kpi_var_tx_noobs_m0, \r\n        diff_devices_live_m2, pct_change_sum_kb_down_noche_m1, prod_cm_count_days_danio_abierto_m0, max_n_continuado_live_m0, sum_tot_importe_arpu_bin_m0, \r\n        max_kpi_var_snr_obs_m0, prop_active_prodpremium_disney_u12m, sum_futbol_premium_time_live_m0, max_kpi_ccer_up_obs_m0, stddev_zt_tel_cli_u3m, \r\n        avg_kb_up_madrugada_m0, sumacum_dias_live_u3m, diff_zt_cm_cli_m1, pct_change_kb_up_maniana_m3, diff_sum_kb_down_tarde_m1, diff_devices_live_m1, \r\n        diff_promos_canceladas_prod_cm_nomain_m1, avg_kpi_cer_up_noobs_m0, count_ah3_d_fin_live_m0, sumacum_cloud_live_u3m, sumacum_zap_live_u6m, diff_zt_tel_m3, \r\n        diff_netflix_live_m1, pct_change_ah4_d_sem_cutv_m1, sum_acum_prodpremium_hbo_u3m, pct_change_ah4_d_fin_live_m1, max_kpi_snr_obs_m0, max_kpi_cer_up_noobs_m0, \r\n        sumacum_time_live_u6m, pct_change_time_cutv_m3, antiq_cm_zt_m0, stddev_zt_cli_viv_u6m, antiq_enabled_dignet_zt_m0, diff_sum_kb_down_madrugada_m3, \r\n        stddev_zt_cli_viv_u3m, stddev_zt_cmdm_u6m, avg_time_cutv_m0, diff_kb_up_tarde_m1, avg_zt_tel_cli_u6m, antiq_enabled_velaccess_zt_m0, stddev_zt_cli_kms_u6m, \r\n        avg_zt_cli_kms_u3m, numacum_promos_fide_prod_cm_main_u3m, tend_kb_up_noche_m3, pct_change_sum_kb_up_maniana_m1, pct_change_duration_call_m1, avg_kpi_ccer_up_noobs_m0, \r\n        diff_futbol_premium_live_m5, prod_flow_sum_days_danio_cerrado_m0, flg_converge_cv, diff_futbol_premium_live_m1, pct_change_tot_tipo_prod_cm_infundado_danio_m3, \r\n        antiq_catv_zt_m0, stddev_zt_tel_cli_u6m, avg_kpi_tx_noobs_u3m, max_ccer_down_m0, avg_kpi_rx_obs_u3m, diff_hbo_premium_live_m5, avg_kb_down_noche_m0,\r\n        tend_kb_down_weekend_m3, diff_promos_fide_prod_cm_main_m1, tend_tot_tipo_prod_cm_cerrado_danio_m3, diff_sum_kb_up_madrugada_m1, diff_stb_cutv_m2, \r\n        count_devices_live_m0, diff_stb_cutv_m1, pct_change_tot_kb_down_m3, pct_change_sum_kb_up_madrugada_m1, tend_kb_down_maniana_m3, tend_vod_stb_time_m3, \r\n        cant_periodos_promo_prod_cm_nomain_m0, pct_change_kb_up_weekend_m1, pct_change_kb_down_tarde_m1, pct_change_futbol_premium_time_live_m1, diff_hbo_premium_live_m1, \r\n        pct_change_ratio_down_up_week_m3, diff_sum_kb_down_tarde_m3, tend_kb_down_noche_m3, sum_tot_tipo_prod_flow_cerrado_danio_m0, diff_sum_kb_down_maniana_m3, \r\n        sumacum_netflix_live_u3m, tend_kb_up_tarde_m3, max_kpi_tx_noobs_u3m, count_ah1_d_sem_cutv_m0, diff_live_m5, sum_acum_tot_tipo_prod_cm_cerrado_danio_u6m, \r\n        diff_live_m2, pct_change_ah4_d_fin_live_m3, diff_promos_vigentes_prod_cm_nomain_m3, sum_tot_promos_arpu_bin_m0, sum_phone_time_live_m0, avg_kpi_cer_up_obs_u3m, \r\n        avg_n_continuado_live_m0, num_days_tot_lastinteract_u3m, prod_cm_antiq, pct_change_kb_down_week_m3, pct_change_time_live_m1, pct_change_sum_kb_down_maniana_m3, \r\n        max_us_ccer_m0, tend_kb_up_maniana_m6, count_ah3_d_sem_live_m0, pct_change_ah1_d_fin_live_m3, diff_disney_live_m2, count_vod_ah1_d_sem_m0, provincia_subregion_encoded,\r\n        diff_kb_up_maniana_m1, diff_sum_kb_down_noche_m3, pct_change_ratio_down_up_tot_m1, count_ah1_d_sem_live_m0, pct_change_programas_cutv_m1, \r\n        diff_tot_tipo_prod_cm_abierto_danio_m1, cat_rrss_full_user_ohe, pct_change_ah3_d_fin_live_m3, tend_kb_down_maniana_m6, pct_change_tih_live_m3,\r\n        pct_change_canales_live_m1, pct_change_zt_tel_m3, avg_kpi_rx_noobs_u3m, avg_zt_tm_u6m, diff_netflix_live_m5, pct_change_importe_saldo_antiq_m1, \r\n        diff_ratio_down_up_weekend_m1, cat_str_full_user_ohe, pct_change_dias_cutv_m1, avg_dur_cumsum_live_m0, diff_zt_cli_kms_m3, diff_device_types_live_m5, \r\n        diff_tot_tipo_prod_cm_infundado_danio_m3, pct_change_ratio_down_up_tot_m3, pct_change_importe_saldo_antiq_m3, avg_kpi_cer_dw_noobs_u3m, \r\n        prod_cm_avg_days_danio_cerrado_m0, max_dur_cumsum_live_m0, avg_kpi_ccer_up_noobs_u3m, stddev_kb_up_weekend_u3m, pct_change_dias_cutv_m3, locales, \r\n        stddev_zt_tel_u6m, max_kpi_cer_up_obs_m0, pct_change_kb_down_week_m1, sumacum_tih_cutv_u6m, pct_change_disney_time_live_m1, count_vod_ah4_d_fin_m0, \r\n        diff_promos_canceladas_prod_cm_nomain_m3, pct_change_kb_down_weekend_m1, tend_zt_tm_m6, antiq_normalmpi_zt_m0, diff_tot_tipo_prod_cm_cerrado_danio_m3,\r\n        ratio_down_up_tot_m0, pct_change_stb_time_cutv_m1, antiq_enabled_serv_zt_m0, max_kpi_ccer_up_obs_u3m, stddev_zt_cm_cli_u3m, \r\n        pct_change_tot_tipo_prod_cm_abierto_danio_m3, tend_tot_tipo_prod_flow_infundado_danio_m3, pct_change_ah2_d_fin_live_m3, sexo_encoded                    \r\n                         \r\n        \r\n        \"\"\"\r\n        \r\nABT_TABLA = \"\"\" data_lake_analytics.abt_churncomboflow_m \"\"\"\r\n\r\nTGT_TABLA = \"\"\" data_lake_analytics.stg_hogartargetschurn_m \"\"\"\r\n\r\nTGT_VARIABLES = \"  target_churn_comboflow \" # Campo que es el Target en la TGT_Tabla\r\n                \r\nTGT_BALENCEO = 20 \r\n\r\nDECIMALES_VARIABLES_NUMERICAS = 3  # 3 es recomendado\r\n\r\nCOTA_CORRELACIONES = 0.8           # 0.8 es recomendado\r\n\r\nPARTICIONES = 8 # SI la tabla es de menos de 200k poner 1 o 2 particiones\r\n                # Si la tabla es grande poner multiplos de 4 \r\n\r\nPORCENTAJE_TRAINING = 0.2\r\n\r\n##################################################################################\r\n\r\nMODELO_PRODUCTIVO = 'RF'  # RF GB LGBM XGB\r\n\r\nMODELO_PRODUCTIVO_param_test ={'numTrees': [20],\r\n              'maxIter': [-999],  # Este no sirve en RF\r\n             'maxDepth':[5],\r\n             'minInstancesPerNode': [10000], \r\n             'maxBins': [10]\r\n             }\r\n             \r\n##################################################################################\r\n# Random Forest\r\n# Parametros Recomendados Maximos\r\n\r\nRF_param_test ={'numTrees': [75, 100, 125],\r\n              'maxIter': [-999],  # Este no sirve en RF\r\n             'maxDepth':[5, 8, 10],\r\n             'minInstancesPerNode': [10000, 20000],\r\n             'maxBins': [10]\r\n             }\r\n\r\n\r\n# Parametros Recomendados Básicos\r\n\r\nRF_param_test ={'numTrees': [20],\r\n              'maxIter': [-999],  # Este no sirve en RF\r\n             'maxDepth':[5],\r\n             'minInstancesPerNode': [10000], \r\n             'maxBins': [10]\r\n             }\r\n\r\n\r\n# Gradient Boosting\r\n# Parametros Recomendados Maximos\r\n\r\nGB_param_test ={'numTrees': [-999], # Este no sirve en GB\r\n              'maxIter': [75, 100, 125],  \r\n             'maxDepth':[5, 8, 10],\r\n             'minInstancesPerNode': [10000, 20000],\r\n             'maxBins': [10]\r\n             }\r\n\r\n# Parametros Recomendados Básicos\r\nGB_param_test ={'numTrees': [-999], # Este no sirve en GB\r\n              'maxIter': [20],  \r\n             'maxDepth':[5],\r\n             'minInstancesPerNode': [10000], \r\n             'maxBins': [10]\r\n             }\r\n             \r\n\r\n# LGBM\r\n# Parametros Recomendados Maximos\r\n\r\nLGBM_param_test ={ #'num_leaves': np.arange(3, 8, 1), \r\n             'min_child_samples': [10000, 20000],\r\n             'max_depth':[5, 8, 10],\r\n             'n_estimators': [75, 100, 125],\r\n             'learning_rate':[0.1]\r\n             }\r\n\r\n# Parametros Recomendados Básicos\r\n\r\nLGBM_param_test ={'min_child_samples': [10000],\r\n             'max_depth':[5],\r\n             'n_estimators': [20]\r\n             }\r\n\r\n\r\n# XGB\r\n# Parametros Recomendados Maximos\r\n\r\nXGB_param_test ={\r\n     'gamma': [0.1],\r\n     'max_depth':[5, 8, 10],\r\n     'n_estimators': [75, 100, 125],\r\n     'learning_rate':[0.1]\r\n     }\r\n     \r\n\r\n# Parametros Recomendados Básicos\r\n\r\nXGB_param_test ={\r\n     'gamma': [0.1],\r\n     'max_depth':[5],\r\n     'n_estimators': [20],\r\n     'learning_rate':[0.0001]\r\n     }","user":"u632820","dateUpdated":"2022-11-25T16:46:24-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_91155<br/>Spark WebUI: <a href=\"null\">null</a>"}]},"apps":[],"jobName":"paragraph_1669230994265_555852599","id":"20221123-161634_1826422979","dateCreated":"2022-11-23T16:16:34-0300","dateStarted":"2022-11-25T16:46:24-0300","dateFinished":"2022-11-25T16:46:25-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"text":"%livy.pyspark\n\n# Ejecuto primero el challenger.... que crea la tabla y entrena los primeros modelos\n\nEjecutarChallenger()","user":"u632820","dateUpdated":"2022-11-24T15:08:19-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Job is cancelled"}]},"apps":[],"jobName":"paragraph_1669292416617_1428457352","id":"20221124-092016_872064414","dateCreated":"2022-11-24T09:20:16-0300","dateStarted":"2022-11-24T09:20:19-0300","dateFinished":"2022-11-24T09:56:52-0300","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%livy.pyspark\n\n# Cambio algunos Hiperparametros.....\n\nCORRER_RF = True\nCORRER_GB = True\nCORRER_LGBM = True\nCORRER_XGB = True\nCORRER_PRODUCTIVO = False  # No lo vuelvo a ejecutar\n\n\n# RF\n\nRF_param_test ={'numTrees': [20],\n              'maxIter': [-999],  # Este no sirve en RF\n             'maxDepth':[7],\n             'minInstancesPerNode': [10000], \n             'maxBins': [10]\n             }\n\n\n# Gradient Boosting\n\nGB_param_test ={'numTrees': [-999], # Este no sirve en GB\n              'maxIter': [20],  \n             'maxDepth':[7],\n             'minInstancesPerNode': [10000], \n             'maxBins': [10]\n             }\n             \n\n# LGBM\n\nLGBM_param_test ={'min_child_samples': [10000],\n             'max_depth':[7],\n             'n_estimators': [20]\n             }\n\n\n# XGB\n\nXGB_param_test ={\n     'gamma': [0.1],\n     'max_depth':[7],\n     'n_estimators': [20],\n     'learning_rate':[0.0001]\n     }\n\nEntrenarModelos(2)  # le cambio el nro de corrida....\n","user":"u632820","dateUpdated":"2022-11-24T15:09:10-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Entrenamiento:- 2022-11-24 13:19:22.081963\n******************************\n RANDOM FOREST \nRANDOM FOREST\nTRAIN Shape:  295820  -  73\nNum. numeric vars:  71\nCon Scaler !!!!!!!!!!!!!!!!!!!!!\n********************\nauc 1  0.6138724646463894\n********************\nauc 2  0.6138724646463773\n********************\n********************\n{Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='featuresCol', doc='features column name'): 'features_scaled', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='labelCol', doc='label column name'): 'label_', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 10, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 10000, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='numTrees', doc='Number of trees to train (>= 1)'): 20, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='seed', doc='random seed'): 12345, Param(parent='RandomForestClassifier_4076a1b471447a46f6c1', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\nTraining\n1.0     29583\n7.0     29583\n3.0     29583\n5.0     29583\n9.0     29583\n10.0    29581\n6.0     29581\n8.0     29581\n4.0     29581\n2.0     29581\nName: decil, dtype: int64\n1.0     2479\n2.0     2036\n3.0     1754\n4.0     1605\n5.0     1405\n6.0     1277\n7.0     1129\n8.0      926\n9.0      773\n10.0     574\nName: decil, dtype: int64\ndecil\n1.0     0.061346\n2.0     0.055633\n3.0     0.051818\n4.0     0.049005\n5.0     0.046659\n6.0     0.043791\n7.0     0.040784\n8.0     0.038228\n9.0     0.033944\n10.0    0.024349\nName: Prob1, dtype: float32\n********************\nTesting\n7     120228\n2     119002\n4     118768\n1     118539\n3     118513\n9     117942\n10    117929\n8     117740\n6     117530\n5     117473\nName: decil, dtype: int64\n1     9483\n2     8111\n3     7202\n4     6447\n5     5706\n6     5178\n7     4619\n8     3773\n9     3130\n10    2501\nName: decil, dtype: int64\nTesting \n********************\nauc 2  0.604883405619715\n********************\nauc 2  0.6014505879190929\n********************\nauc 2  0.5955878961298496\n******************************\n GRADIENT BOOSTING \nGradient BOOSTING\nTRAIN Shape:  295841  -  73\nNum. numeric vars:  71\nCon Scaler !!!!!!!!!!!!!!!!!!!!!\n********************\nauc 1  0.6222966295713825\n********************\nauc 2  0.6222966295713804\n********************\n********************\n{Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='featuresCol', doc='features column name'): 'features_scaled', Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='labelCol', doc='label column name'): 'label_', Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic', Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 10, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='maxIter', doc='maximum number of iterations (>= 0)'): 20, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 10000, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='seed', doc='random seed'): 12345, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='GBTClassifier_487c9ab485c57764fa7a', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\nTraining\n5.0     30247\n8.0     29664\n10.0    29588\n1.0     29585\n3.0     29585\n4.0     29584\n2.0     29583\n9.0     29579\n7.0     29505\n6.0     28921\nName: decil, dtype: int64\n1.0     2607\n2.0     2070\n3.0     1837\n4.0     1502\n5.0     1478\n6.0     1202\n7.0     1056\n8.0      963\n9.0      787\n10.0     528\nName: decil, dtype: int64\ndecil\n1.0     0.087725\n2.0     0.082063\n3.0     0.078193\n4.0     0.074060\n5.0     0.069782\n6.0     0.067177\n7.0     0.064399\n8.0     0.061418\n9.0     0.058697\n10.0    0.049457\nName: Prob1, dtype: float32\n********************\nTesting\n5     121226\n3     119791\n4     118535\n2     118238\n1     118049\n8     118037\n7     118013\n9     117887\n10    117348\n6     116519\nName: decil, dtype: int64\n1     9951\n2     8258\n3     7405\n4     6154\n5     5762\n6     5063\n7     4296\n8     3913\n9     3226\n10    2050\nName: decil, dtype: int64\nTesting \n********************\nauc 2  0.607391521745446\n********************\nauc 2  0.6073618586389412\n********************\nauc 2  0.6009550245227493\n******************************\n LIGHTGBM \nTRAIN Shape:  295841  -  73\ncolumnas  71\nCon Scaler !!!!!!!!!!!!!!!!!!!!!\n['avg_kb_down_noche_m0', 'avg_kb_up_noche_m0', 'avg_kb_up_weekend_u6m', 'avg_kpi_ccer_up_noobs_m0', 'avg_kpi_ccer_up_noobs_u3m', 'avg_kpi_cer_dw_noobs_u3m', 'avg_kpi_cer_up_noobs_m0', 'avg_kpi_cer_up_obs_u3m', 'avg_kpi_mer_noobs_u3m', 'avg_kpi_rx_noobs_u3m', 'avg_kpi_rx_obs_u3m', 'avg_kpi_tx_noobs_u3m', 'avg_kpi_var_tx_noobs_m0', 'avg_rxpower_down_noche_m0', 'cat_gam_heavy_user_ohe', 'count_ah1_d_sem_cutv_m0', 'count_ah1_d_sem_live_m0', 'count_ah3_d_fin_live_m0', 'count_vod_ah1_d_sem_m0', 'count_vod_ah4_d_fin_m0', 'countacum_pagos_electronico_u6m', 'diff_devices_live_m2', 'diff_futbol_premium_live_m5', 'diff_promos_canceladas_prod_cm_nomain_m1', 'diff_stb_cutv_m2', 'diff_sum_kb_down_tarde_m1', 'diff_zt_tel_cli_m1', 'flg_converge_cv', 'locales', 'max_dur_cumsum_cutv_m0', 'max_kpi_ccer_up_obs_m0', 'max_kpi_ccer_up_obs_u3m', 'max_kpi_cer_up_noobs_m0', 'max_kpi_cer_up_obs_m0', 'max_kpi_snr_obs_m0', 'max_kpi_tx_noobs_u3m', 'max_kpi_var_snr_obs_m0', 'pct_change_ah1_d_fin_live_m1', 'pct_change_ah1_d_fin_live_m3', 'pct_change_ah2_d_fin_live_m3', 'pct_change_ah2_d_sem_live_m1', 'pct_change_ah3_d_sem_live_m1', 'pct_change_ah4_d_fin_live_m1', 'pct_change_disney_time_live_m1', 'pct_change_duration_call_m1', 'pct_change_futbol_premium_time_live_m1', 'pct_change_importe_saldo_antiq_m1', 'pct_change_importe_saldo_antiq_m3', 'pct_change_kb_down_tarde_m1', 'pct_change_kb_down_week_m3', 'pct_change_kb_down_weekend_m1', 'pct_change_kb_up_maniana_m3', 'pct_change_kb_up_tarde_m3', 'pct_change_kb_up_total_m1', 'pct_change_phone_time_live_m1', 'pct_change_sum_kb_down_noche_m1', 'pct_change_time_live_m1', 'pct_change_tot_kb_down_m3', 'prod_cm_antiq', 'sum_acum_tot_prodpremium_u6m', 'sum_acum_tot_tipo_prod_flow_cerrado_danio_u3m', 'sum_futbol_premium_time_live_m0', 'sum_hbo_premium_time_live_m0', 'sum_kb_up_maniana_m0', 'sum_phone_time_live_m0', 'sum_tot_importe_arpu_bin_m0', 'sumacum_cloud_live_u3m', 'sumacum_time_live_u6m', 'tend_tot_kb_up_m3', 'tend_vod_time_m6', 'zt_tel_m0']\nLGBM\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.620055\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619387\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619105\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.618453\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619977\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619695\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619599\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.620183\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619605\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.62016\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTraining until validation scores don't improve for 1000 rounds\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.620266\n{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_samples': 10000, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 20, 'n_jobs': 4, 'num_leaves': 31, 'objective': None, 'random_state': 314, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'metric': 'None'}\n{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': 4, 'num_leaves': 31, 'objective': None, 'random_state': 314, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'metric': 'None'}\nTraining until validation scores don't improve for 1000 rounds\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nDid not meet early stopping. Best iteration is:\n[20]\tvalid_0's auc: 0.619705\nROC:  0.6197051783366238\nTraining\n9.0     20763\n3.0     20713\n1.0     20710\n5.0     20710\n7.0     20709\n8.0     20708\n6.0     20708\n2.0     20708\n4.0     20704\n10.0    20655\nName: decil, dtype: int64\n1.0     1930\n2.0     1433\n3.0     1276\n4.0     1130\n5.0      940\n6.0      844\n7.0      749\n8.0      643\n9.0      527\n10.0     349\nName: decil, dtype: int64\ndecil\n1.0     0.067430\n2.0     0.061061\n3.0     0.056126\n4.0     0.051070\n5.0     0.045430\n6.0     0.041295\n7.0     0.037198\n8.0     0.033499\n9.0     0.029671\n10.0    0.022473\nName: Prob1, dtype: float64\n********************\nTesting\n6     9011\n7     8936\n2     8929\n8     8918\n9     8854\n4     8849\n10    8848\n5     8826\n3     8791\n1     8791\nName: decil, dtype: int64\n1     768\n2     614\n3     527\n4     440\n5     421\n6     373\n7     360\n8     286\n9     234\n10    186\nName: decil, dtype: int64\nTesting \n(1092792, 71)\n71\nLGBM\nROC :  0.6048217991880097\n(1121859, 71)\n71\nLGBM\nROC :  0.6053753818441616\n(1140305, 71)\n71\nLGBM\nROC :  0.5983556740936962\n******************************\n XGB \nTRAIN Shape:  295841  -  73\ncolumnas  71\nCon Scaler !!!!!!!!!!!!!!!!!!!!!\n['avg_kb_down_noche_m0', 'avg_kb_up_noche_m0', 'avg_kb_up_weekend_u6m', 'avg_kpi_ccer_up_noobs_m0', 'avg_kpi_ccer_up_noobs_u3m', 'avg_kpi_cer_dw_noobs_u3m', 'avg_kpi_cer_up_noobs_m0', 'avg_kpi_cer_up_obs_u3m', 'avg_kpi_mer_noobs_u3m', 'avg_kpi_rx_noobs_u3m', 'avg_kpi_rx_obs_u3m', 'avg_kpi_tx_noobs_u3m', 'avg_kpi_var_tx_noobs_m0', 'avg_rxpower_down_noche_m0', 'cat_gam_heavy_user_ohe', 'count_ah1_d_sem_cutv_m0', 'count_ah1_d_sem_live_m0', 'count_ah3_d_fin_live_m0', 'count_vod_ah1_d_sem_m0', 'count_vod_ah4_d_fin_m0', 'countacum_pagos_electronico_u6m', 'diff_devices_live_m2', 'diff_futbol_premium_live_m5', 'diff_promos_canceladas_prod_cm_nomain_m1', 'diff_stb_cutv_m2', 'diff_sum_kb_down_tarde_m1', 'diff_zt_tel_cli_m1', 'flg_converge_cv', 'locales', 'max_dur_cumsum_cutv_m0', 'max_kpi_ccer_up_obs_m0', 'max_kpi_ccer_up_obs_u3m', 'max_kpi_cer_up_noobs_m0', 'max_kpi_cer_up_obs_m0', 'max_kpi_snr_obs_m0', 'max_kpi_tx_noobs_u3m', 'max_kpi_var_snr_obs_m0', 'pct_change_ah1_d_fin_live_m1', 'pct_change_ah1_d_fin_live_m3', 'pct_change_ah2_d_fin_live_m3', 'pct_change_ah2_d_sem_live_m1', 'pct_change_ah3_d_sem_live_m1', 'pct_change_ah4_d_fin_live_m1', 'pct_change_disney_time_live_m1', 'pct_change_duration_call_m1', 'pct_change_futbol_premium_time_live_m1', 'pct_change_importe_saldo_antiq_m1', 'pct_change_importe_saldo_antiq_m3', 'pct_change_kb_down_tarde_m1', 'pct_change_kb_down_week_m3', 'pct_change_kb_down_weekend_m1', 'pct_change_kb_up_maniana_m3', 'pct_change_kb_up_tarde_m3', 'pct_change_kb_up_total_m1', 'pct_change_phone_time_live_m1', 'pct_change_sum_kb_down_noche_m1', 'pct_change_time_live_m1', 'pct_change_tot_kb_down_m3', 'prod_cm_antiq', 'sum_acum_tot_prodpremium_u6m', 'sum_acum_tot_tipo_prod_flow_cerrado_danio_u3m', 'sum_futbol_premium_time_live_m0', 'sum_hbo_premium_time_live_m0', 'sum_kb_up_maniana_m0', 'sum_phone_time_live_m0', 'sum_tot_importe_arpu_bin_m0', 'sumacum_cloud_live_u3m', 'sumacum_time_live_u6m', 'tend_tot_kb_up_m3', 'tend_vod_time_m6', 'zt_tel_m0']\nXGBoost\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8, 'gamma': 0.1, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.0001, 'max_delta_step': 0, 'max_depth': 5, 'min_child_weight': 0.6, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 20, 'n_jobs': 0, 'num_parallel_tree': 1, 'random_state': 1234, 'reg_alpha': 0.11, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 0.85, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'seed': 1234}\nROC:  0.6116144186833161\nTraining\n9.0     22790\n6.0     20968\n4.0     20879\n2.0     20838\n10.0    20802\n5.0     20709\n1.0     20623\n3.0     20497\n7.0     19850\n8.0     19132\nName: decil, dtype: int64\n1.0     1922\n2.0     1398\n3.0     1226\n4.0     1162\n5.0      913\n6.0      846\n7.0      752\n9.0      618\n8.0      595\n10.0     389\nName: decil, dtype: int64\ndecil\n1.0     0.499134\n2.0     0.499120\n3.0     0.499112\n4.0     0.499106\n5.0     0.499091\n6.0     0.499081\n7.0     0.499075\n8.0     0.499067\n9.0     0.499064\n10.0    0.499050\nName: Prob1, dtype: float32\n********************\nTesting\n9     9741\n6     9152\n4     8959\n2     8949\n10    8903\n3     8818\n5     8761\n1     8743\n7     8551\n8     8176\nName: decil, dtype: int64\n1     694\n2     631\n3     510\n4     497\n5     410\n6     393\n7     326\n8     288\n9     272\n10    188\nName: decil, dtype: int64\nTesting \n(1092792, 71)\n71\nXGBoost\nROC :  0.602204853714144\n(1121859, 71)\n71\nXGBoost\nROC :  0.6021973569434701\n(1140305, 71)\n71\nXGBoost\nROC :  0.5965769498344105\n******************************\n MODELO PRODUCTIVO \nRANDOM FOREST\nTRAIN Shape:  295841  -  73\nNum. numeric vars:  71\nCon Scaler !!!!!!!!!!!!!!!!!!!!!\n********************\nauc 1  0.6142440095528511\n********************\nauc 2  0.614244009552856\n********************\n********************\n{Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='featuresCol', doc='features column name'): 'features_scaled', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='labelCol', doc='label column name'): 'label_', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 10, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 10000, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='numTrees', doc='Number of trees to train (>= 1)'): 20, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='seed', doc='random seed'): 12345, Param(parent='RandomForestClassifier_4ebcab951906eb41ee65', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\nTraining\n1.0     29585\n7.0     29584\n6.0     29584\n10.0    29584\n3.0     29584\n5.0     29584\n4.0     29584\n9.0     29584\n2.0     29584\n8.0     29584\nName: decil, dtype: int64\n1.0     2485\n2.0     1966\n3.0     1794\n4.0     1641\n5.0     1442\n6.0     1205\n7.0     1132\n8.0      946\n9.0      779\n10.0     640\nName: decil, dtype: int64\ndecil\n1.0     0.060188\n2.0     0.055522\n3.0     0.051926\n4.0     0.049024\n5.0     0.046518\n6.0     0.043924\n7.0     0.041131\n8.0     0.038283\n9.0     0.035215\n10.0    0.025851\nName: Prob1, dtype: float32\n********************\nTesting\n4     119764\n6     119031\n3     118759\n5     118291\n7     118289\n2     118205\n8     118119\n9     118034\n1     117754\n10    117397\nName: decil, dtype: int64\n1     9491\n2     8162\n3     7097\n4     6447\n5     5804\n6     5098\n7     4420\n8     3813\n9     3309\n10    2437\nName: decil, dtype: int64\nTesting \n********************\nauc 2  0.6028236915434305\n********************\nauc 2  0.601405975423542\n********************\nauc 2  0.5971947971603175\nEntrenamiento Fin:- 2022-11-24 14:10:04.968124\n/usr/local/lib64/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  % (grid_size, self.n_iter, grid_size), UserWarning)\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   22.5s finished\n<string>:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n<string>:49: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/usr/local/lib64/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  % (grid_size, self.n_iter, grid_size), UserWarning)\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   47.5s finished\n<string>:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n<string>:49: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669306610777_1467968873","id":"20221124-131650_1122838924","dateCreated":"2022-11-24T13:16:50-0300","dateStarted":"2022-11-24T13:19:22-0300","dateFinished":"2022-11-24T14:11:14-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"text":"%livy.pyspark\nspark.sql(' select * from sdb_datamining.' + modelo + '_metricas' ).show()","user":"u632820","dateUpdated":"2022-11-24T15:03:05-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+---------------+--------------------+-------+--------------------+\n|           algorithm|    metric_desc|        metric_value|periodo|                 bin|\n+--------------------+---------------+--------------------+-------+--------------------+\n|               XGB_2|hyperparametros|{'objective': 'bi...| 202211|202211_challenger...|\n|               XGB_1|hyperparametros|{'objective': 'bi...| 202211|202211_challenger...|\n|              LGBM_2|hyperparametros|{'boosting_type':...| 202211|202211_challenger...|\n|              LGBM_1|hyperparametros|{'boosting_type':...| 202211|202211_challenger...|\n|      RF_PRODUCTIVO2|hyperparametros|{Param(parent='Ra...| 202211|202211_challenger...|\n|       RF_PRODUCTIVO|hyperparametros|{Param(parent='Ra...| 202211|202211_challenger...|\n|                RF_2|hyperparametros|{Param(parent='Ra...| 202211|202211_challenger...|\n|                RF_1|hyperparametros|{Param(parent='Ra...| 202211|202211_challenger...|\n|                GB_2|hyperparametros|{Param(parent='GB...| 202211|202211_challenger...|\n|                GB_1|hyperparametros|{Param(parent='GB...| 202211|202211_challenger...|\n|RF_PRODUCTIVO2 TE...|     AUC_TESTEO|  0.6028236915434305| 202211|202211_challenger...|\n|RF_PRODUCTIVO2 TE...|     AUC_TESTEO|  0.5971947971603175| 202211|202211_challenger...|\n|RF_PRODUCTIVO2 TE...|     AUC_TESTEO|   0.601405975423542| 202211|202211_challenger...|\n|RF_PRODUCTIVO TES...|     AUC_TESTEO|  0.6048834056197266| 202211|202211_challenger...|\n|RF_PRODUCTIVO TES...|     AUC_TESTEO|  0.5955878961298414| 202211|202211_challenger...|\n|RF_PRODUCTIVO TES...|     AUC_TESTEO|  0.6014505879191302| 202211|202211_challenger...|\n| LGBM_1 TESTING MES3|     AUC_TESTEO|  0.5983556740936962| 202211|202211_challenger...|\n| LGBM_1 TESTING MES2|     AUC_TESTEO|  0.6053753818441616| 202211|202211_challenger...|\n| LGBM_2 TESTING MES1|     AUC_TESTEO|  0.6048217991880097| 202211|202211_challenger...|\n| LGBM_2 TESTING MES3|     AUC_TESTEO|  0.5983556740936962| 202211|202211_challenger...|\n+--------------------+---------------+--------------------+-------+--------------------+\nonly showing top 20 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1668539334092_75073<br/>Spark WebUI: <a href=\"http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/\">http://sr-hadctl-xp03.corp.cablevision.com.ar:8088/proxy/application_1668539334092_75073/</a>"}]},"apps":[],"jobName":"paragraph_1669292648661_1800395838","id":"20221124-092408_640495252","dateCreated":"2022-11-24T09:24:08-0300","dateStarted":"2022-11-24T15:03:05-0300","dateFinished":"2022-11-24T15:04:17-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%livy.pyspark\n\n#Elijo el mejor modelo.... y veo si performa mejor que el de produccion\n\nMejorModeloEntrenado()","user":"u632820","dateUpdated":"2022-11-24T15:09:02-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1669313303330_1659174745","id":"20221124-150823_1464872693","dateCreated":"2022-11-24T15:08:23-0300","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"text":"%livy.pyspark\n\nBorrarTablasTemporales()","user":"u632820","dateUpdated":"2022-11-23T16:11:37-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1669228378544_-1211493733","id":"20221107-135340_1850327813","dateCreated":"2022-11-23T15:32:58-0300","dateStarted":"2022-11-23T16:11:37-0300","dateFinished":"2022-11-23T16:11:38-0300","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"","user":"u632820","dateUpdated":"2022-11-23T16:11:38-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1669228378727_1462111182","id":"20221107-135202_1194084027","dateCreated":"2022-11-23T15:32:58-0300","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:207"}],"name":"_LAB/sbarbone/Challenger Clase","id":"2HJUFQU8K","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"jdbc:shared_process":[],"sparkHWC:shared_process":[],"spark:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":true,"looknfeel":"default","personalizedMode":"false"},"info":{}}